{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese archhitecture with triplet loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare images to use it in the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def fetch(img_dir, name):\n",
    "    #print('image ' + str(name))\n",
    "    img = cv2.imread(join(img_dir, name))\n",
    "    if img.shape == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    elif img.shape == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize(img, size=(1024, 768)):\n",
    "    assert len(size) == 2\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def pad(img, size=(1024, 768)):\n",
    "    assert len(img.shape) == 3\n",
    "    assert len(size) == 2\n",
    "    h, w, _ = img.shape\n",
    "    #assert w <= size[0] and h <= size[1]\n",
    "    pad_vert = np.ceil((size[1]-h) / 2).astype(np.uint32)\n",
    "    pad_hor = np.ceil((size[0]-w) / 2).astype(np.uint32)\n",
    "\n",
    "    padded = np.full((size[1], size[0], 3), 255).astype(np.uint8)\n",
    "    padded[pad_vert:pad_vert+h, pad_hor:pad_hor+w, :] = img.copy()\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words sequence class that uses data preparation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "class WordsSequence(Sequence):\n",
    "    def __init__(self, img_dir, input_shape, x_set, y_set=None, batch_size=16):\n",
    "        if y_set is not None:\n",
    "            self.x, self.y = x_set, y_set\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "        else:\n",
    "            self.x, self.y = x_set, None\n",
    "            \n",
    "        self.img_dir = img_dir\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x])\n",
    "\n",
    "        unused = self.dataset.loc[self.dataset['used'] == 0]\n",
    "            \n",
    "        if len(unused) >= self.batch_size:\n",
    "            batch_indices = unused.sample(n=self.batch_size).index\n",
    "        else:\n",
    "            batch_indices = unused.sample(n=self.batch_size, replace=True).index\n",
    "\n",
    "        self.dataset.loc[batch_indices, 'used'] = 1\n",
    "        batch_x = self.dataset.iloc[batch_indices]['x'].values\n",
    "        batch_y = self.dataset.iloc[batch_indices]['y'].values\n",
    "        return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        assert len(img.shape) == 3\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        if h / w <= self.input_shape[0] / self.input_shape[1]:\n",
    "            img = resize(img, (self.input_shape[1], int(self.input_shape[1] * h / w)))\n",
    "        else:\n",
    "            img = resize(img, (int(self.input_shape[0] * w / h), self.input_shape[0]))\n",
    "\n",
    "        img = pad(img, (self.input_shape[1], self.input_shape[0]))\n",
    "        return img / 255.  \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.y is not None:\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplets loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def valid_triplets_mask(labels):\n",
    "    \"\"\"Compute the 3D boolean mask where mask[a, p, n] is True if (a, p, n) is a valid triplet,\n",
    "    as in a, p, n are distinct and labels[a] == labels[p], labels[a] != labels[n].\n",
    "\n",
    "    :param labels: tensor of shape (batch_size,)\n",
    "    :return mask: tf.bool tensor of shape (batch_size, batch_size, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def euclidean_distance(embeddings, squared=False):\n",
    "    \"\"\"Computes pairwise euclidean distance matrix with numerical stability.\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    :param embeddings: 2-D Tensor of size [number of data, feature dimension].\n",
    "    :param squared: Boolean, whether or not to square the pairwise distances.\n",
    "    :return dist: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    dist_squared = tf.add(tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True),\n",
    "                          tf.reduce_sum(tf.square(tf.transpose(embeddings)), axis=0, keepdims=True)\n",
    "                          ) - 2.0 * tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    dist_squared = tf.maximum(dist_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = tf.less_equal(dist_squared, 0.0)\n",
    "    # Optionally take the sqrt.\n",
    "    dist = dist_squared if squared else tf.sqrt(dist_squared + tf.cast(error_mask, dtype=tf.float32) * 1e-16)\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    dist = tf.multiply(dist, tf.cast(tf.logical_not(error_mask), dtype=tf.float32))\n",
    "\n",
    "    n_data = tf.shape(embeddings)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = tf.ones_like(dist) - tf.linalg.diag(tf.ones([n_data]))\n",
    "    dist = tf.multiply(dist, mask_offdiagonals)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the maximum.\n",
    "    :return masked_maximums: N-D `Tensor`. The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = tf.reduce_min(data, axis=dim, keepdims=True)\n",
    "    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), axis=dim, keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the minimum.\n",
    "    :return masked_minimums: N-D `Tensor`. The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = tf.reduce_max(data, axis=dim, keepdims=True)\n",
    "    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), axis=dim, keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "\n",
    "\n",
    "def triplet_loss(margin=1.0, strategy='batch_semi_hard'):\n",
    "    \"\"\"Compute the triplet loss over the batch of embeddings. tf contrib inspired:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/losses/python/metric_learning/metric_loss_ops.py\n",
    "\n",
    "    :param margin: margin that is going to be enforced by the triplet loss\n",
    "    :param strategy: string, that indicated whether we're using the 'batch hard', 'batch all' or 'batch_semi_hard' mining strategy\n",
    "    :return: a callback function that calculates the loss according to the specified strategy\n",
    "    \"\"\"\n",
    "    def get_loss_tensor(positive_dists, negative_dists):\n",
    "        \"\"\"Compute the triplet loss function tensor using specified margin:\n",
    "\n",
    "        :param positive_dists: positive distances tensor\n",
    "        :param negative_dists:  negative distances tensor\n",
    "        :return: resulting triplet loss tensor\n",
    "        \"\"\"\n",
    "        if margin == 'soft':\n",
    "            return tf.nn.softplus(positive_dists - negative_dists)\n",
    "\n",
    "        return tf.maximum(positive_dists - negative_dists + margin, 0.0)\n",
    "\n",
    "    def batch_semi_hard(labels, embeddings):\n",
    "        \"\"\"Computes the triplet loss with semi-hard negative mining.\n",
    "        The loss encourages the positive distances (between a pair of embeddings with\n",
    "        the same labels) to be smaller than the minimum negative distance among\n",
    "        which are at least greater than the positive distance plus the margin constant\n",
    "        (called semi-hard negative) in the mini-batch. If no such negative exists,\n",
    "        uses the largest negative distance instead.\n",
    "        See: https://arxiv.org/abs/1503.03832.\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        batch_size = tf.size(labels)\n",
    "        # Build pairwise squared distance matrix.\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        # Build pairwise binary adjacency matrix (equal label mask).\n",
    "        adjacency = tf.equal(labels, tf.transpose(labels))\n",
    "        # Invert so we can select negatives only.\n",
    "        adjacency_not = tf.logical_not(adjacency)\n",
    "\n",
    "        # Compute the mask.\n",
    "        dist_tile = tf.tile(dist, [batch_size, 1])  # stack dist matrix batch_size times, axis=0\n",
    "        mask = tf.logical_and(tf.tile(adjacency_not, [batch_size, 1]), tf.greater(dist_tile, tf.reshape(dist, [-1, 1])))\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        is_negatives_outside = tf.reshape(tf.greater(tf.reduce_sum(mask, axis=1, keepdims=True), 0.0), [batch_size, batch_size])\n",
    "        is_negatives_outside = tf.transpose(is_negatives_outside)\n",
    "\n",
    "        # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "        negatives_outside = tf.reshape(masked_minimum(dist_tile, mask), [batch_size, batch_size])\n",
    "        negatives_outside = tf.transpose(negatives_outside)\n",
    "\n",
    "        # negatives_inside: largest D_an.\n",
    "        adjacency_not = tf.cast(adjacency_not, dtype=tf.float32)\n",
    "        negatives_inside = tf.tile(masked_maximum(dist, adjacency_not), [1, batch_size])\n",
    "\n",
    "        semi_hard_negatives = tf.where(is_negatives_outside, negatives_outside, negatives_inside)\n",
    "\n",
    "        # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "        #   in semihard, they take all positive pairs except the diagonal.\n",
    "        mask_positives = tf.cast(adjacency, dtype=tf.float32) - tf.linalg.diag(tf.ones([batch_size]))\n",
    "        n_positives = tf.reduce_sum(mask_positives)\n",
    "\n",
    "        loss_mat = get_loss_tensor(dist, semi_hard_negatives)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(tf.multiply(loss_mat, mask_positives)), n_positives)\n",
    "        return loss\n",
    "\n",
    "    def batch_all(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating all the valid triplets and averaging over the positive ones\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        #mask = tf.to_float(valid_triplets_mask(labels))\n",
    "        mask = tf.cast(valid_triplets_mask(labels), dtype=tf.float32)\n",
    "\n",
    "        anchor_positive_dist = tf.expand_dims(dist, 2)\n",
    "        anchor_negative_dist = tf.expand_dims(dist, 1)\n",
    "\n",
    "        loss_tensor = get_loss_tensor(anchor_positive_dist, anchor_negative_dist)\n",
    "        loss_tensor = tf.multiply(loss_tensor, mask)\n",
    "\n",
    "        #num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_tensor, 1e-16)))\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.cast(tf.greater(loss_tensor, 1e-16), dtype=tf.float32))\n",
    "        #loss = tf.div_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    def batch_hard(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating only hardest valid triplets and averaging over the positive ones.\n",
    "        One triplet per embedding, i.e. per anchor\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        adjacency = tf.cast(tf.equal(tf.reshape(labels, (-1, 1)), tf.reshape(labels, (1, -1))), tf.float32)\n",
    "\n",
    "        pos_dist = tf.reduce_max(adjacency * dist, axis=1)\n",
    "        inf = tf.constant(1e+9, tf.float32)\n",
    "        neg_dist = tf.reduce_min((adjacency * inf) + dist, axis=1)\n",
    "\n",
    "        loss_mat = get_loss_tensor(pos_dist, neg_dist)\n",
    "\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_mat, 1e-16)))\n",
    "        loss = tf.div_no_nan(tf.reduce_sum(loss_mat), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    if strategy == 'batch_semi_hard':\n",
    "        return batch_semi_hard\n",
    "    elif strategy == 'batch hard':\n",
    "        return batch_hard\n",
    "    else:\n",
    "        return batch_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import  sample\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "\n",
    "\n",
    "def get_str2numb_numb2dict(vect):\n",
    "    str_to_ind_dict = {}\n",
    "    count = 0\n",
    "    for v in vect:\n",
    "        if v not in str_to_ind_dict.keys():\n",
    "            str_to_ind_dict[v] = count\n",
    "            count += 1\n",
    "    reverse_dict = {v:k for k, v in str_to_ind_dict.items()}\n",
    "    return str_to_ind_dict, reverse_dict\n",
    "\n",
    "def apply_dict(dict_keys, X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        res.append(dict_keys[x])\n",
    "    return res\n",
    "\n",
    "class ProgbarLossLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super(ProgbarLossLogger, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.seen = 0\n",
    "        self.target = self.params['steps']\n",
    "\n",
    "        if self.epochs > 1:\n",
    "            print('Epoch %d/%d' % (epoch + 1, self.epochs))\n",
    "        self.progbar = Progbar(target=self.target, verbose=True, stateful_metrics=['loss'])\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.seen < self.target:\n",
    "            self.log_values = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        num_steps = logs.get('num_steps', 1)\n",
    "        self.seen += num_steps\n",
    "\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.log_values.append((k, logs[k]))\n",
    "        self.progbar.update(self.seen, self.log_values)\n",
    "        \n",
    "class TripletModel:\n",
    "    def __init__(self, alpha, input_shape, cache_dir):\n",
    "        self.alpha = alpha\n",
    "        self.input_shape = input_shape\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self.model = self.build_model()\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        base_network = MobileNet(input_shape=self.input_shape, alpha=self.alpha, weights='imagenet', include_top=False, \n",
    "                                 pooling='avg')\n",
    "        x = Dense(128)(base_network.output)\n",
    "        x = Lambda(lambda x: K.l2_normalize(x, axis=1))(x)\n",
    "        model = Model(inputs=base_network.input, outputs=x)\n",
    "        model.summary()\n",
    "        return model\n",
    "           \n",
    "    def train(self, train_dir, train_csv, validation_dir, validation_csv, epochs, batch_size=32, learning_rate=0.001, margin=0.5):\n",
    "        train = pd.read_csv(train_csv)\n",
    "        # validation = pd.read_csv(validation_csv)\n",
    "        x_train, y_train = train['file_name'].as_matrix(), train['label'].as_matrix()\n",
    "        # x_validation, y_validation = validation['file_name'].as_matrix(), validation['label'].as_matrix()\n",
    "        \n",
    "        str2ind_train_dict, ind2str_train_dict = get_str2numb_numb2dict(y_train)\n",
    "        y_train = np.array(apply_dict(str2ind_train_dict, y_train))\n",
    "\n",
    "        # str2ind_val_dict, ind2str_val_dict = get_str2numb_numb2dict(y_validation)\n",
    "        # y_validation = np.array(apply_dict(str2ind_val_dict, y_validation))\n",
    "        \n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        train_generator = WordsSequence(train_dir, input_shape=self.input_shape, x_set=x_train, y_set=y_train, batch_size=batch_size)\n",
    "        # validation_generator = WordsSequence(validation_dir, input_shape=self.input_shape, x_set=validation_pairs, y_set=validation_y, batch_size=batch_size)\n",
    "\n",
    "        # optimize = RMSprop(lr=learning_rate)\n",
    "        optimize = Adam(lr=0.00001)\n",
    "        self.model.summary()\n",
    "        self.model.compile(loss=triplet_loss.triplet_loss(margin=1.0, strategy=\"batch_all\"), optimizer=optimize)\n",
    "        \n",
    "        # validation_data=validation_generator, \n",
    "        self.model.fit_generator(train_generator, shuffle=True, epochs=epochs, verbose=1, \n",
    "        callbacks=[ModelCheckpoint(filepath=os.path.join(self.cache_dir, 'checkpoint-{epoch:02d}.h5'), save_weights_only=True)])\n",
    "        \n",
    "        self.model.save('final_model.h5')\n",
    "        self.save_weights('final_weights.h5')\n",
    "\n",
    "\n",
    "    def save_embeddings(self, filename):\n",
    "        self.embeddings.to_pickle(filename)\n",
    "    \n",
    "    def load_embeddings(self, filename):\n",
    "        self.embeddings = pd.read_pickle(filename)    \n",
    "        \n",
    "    def save_weights(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        self.model.load_weights(filename, by_name=True, skip_mismatch=True)\n",
    "        \n",
    "    \n",
    "    def make_embeddings(self, img_dir, csv, batch_size=32):\n",
    "        if self.embeddings is not None:\n",
    "            print(self.embeddings[0][0])\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "        else:\n",
    "            data = pd.read_csv(csv)\n",
    "            x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "            \n",
    "            self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "            y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "            words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "            pred = self.model.predict_generator(words, verbose=1)\n",
    "\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(pred, y) \n",
    "     \n",
    "            self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "            self.save_embeddings('embeddings.pkl')\n",
    "    \n",
    "    def predict(self, img_dir, test_csv, batch_size=32):\n",
    "        self.model.summary()\n",
    "        test = pd.read_csv(test_csv)\n",
    "        x_test, y_test = test['file_name'].as_matrix(), test['label'].as_matrix()\n",
    "        \n",
    "        str2ind_test_dict, ind2str_test_dict = get_str2numb_numb2dict(y_test)\n",
    "        test_y = np.array(apply_dict(str2ind_test_dict, y_test))\n",
    "\n",
    "        words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x_test, batch_size=batch_size)\n",
    "        test_embeddings = self.model.predict_generator(words, verbose=1)\n",
    " \n",
    "        res = self.clf.predict(test_embeddings) \n",
    "        predict = np.array(apply_dict(ind2str_test_dict , res))\n",
    "        count = 0\n",
    "        for i,j in zip(predict, y_test):\n",
    "            if i == j:\n",
    "                count += 1\n",
    "\n",
    "        print('word accuracy: ', count / len(y_test))\n",
    "        \n",
    "        count = 0\n",
    "        autors = np.unique(y_test)\n",
    "        autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "            if p == autors[i]:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    "        \n",
    "        count = 0\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = [pair[0] for pair in Counter(np.ravel(predict[inds])).most_common(5)]\n",
    "            if autors[i] in p:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
